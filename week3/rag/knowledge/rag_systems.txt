检索增强生成（RAG）是一种结合了检索系统和生成式AI的混合架构，旨在提高大型语言模型（LLM）生成内容的质量和准确性。

RAG系统的核心组件包括：
1. 知识库：包含各种文档、文章或其他形式的信息
2. 嵌入模型：将文本转换为向量表示
3. 向量数据库：存储和索引文档的向量表示
4. 检索器：根据查询找到最相关的文档
5. 生成模型：基于检索到的信息生成回答

RAG的工作流程如下：
1. 索引阶段：文档被分割成小块，通过嵌入模型转换为向量，并存储在向量数据库中
2. 查询阶段：用户提问被转换为向量，与向量数据库中的文档进行相似度比较
3. 检索阶段：系统检索出与查询最相关的文档片段
4. 生成阶段：将检索到的文档与原始查询一起发送给LLM，生成最终回答

RAG相比于纯生成式模型的优势：
1. 更新知识更容易：只需更新知识库，无需重新训练模型
2. 减少幻觉：提供事实依据，减少模型编造信息的可能性
3. 可溯源：可以追踪回答的来源文档
4. 更低的计算成本：比完全微调模型更经济

RAG系统的常见挑战：
1. 检索质量：如果检索的文档不相关，会导致生成质量下降
2. 上下文长度限制：LLM的上下文窗口有限，限制了可以提供的文档数量
3. 文档分块策略：如何最佳地分割文档以保留语义完整性
4. 向量相似度的局限性：语义相似不总是意味着内容相关

RAG的高级技术：
1. 混合检索：结合关键词搜索和向量搜索
2. 重排序：使用更复杂的模型对初步检索结果进行重新排序
3. 查询扩展：扩展原始查询以提高检索质量
4. 多跳推理：通过多次检索解决复杂问题

在AWS环境中实现RAG系统的常用组件：
1. Amazon Bedrock：提供Claude、Titan等LLM和嵌入模型
2. Amazon OpenSearch：可作为向量数据库
3. Amazon S3：存储原始文档
4. AWS Lambda：处理检索和生成逻辑
5. Amazon SageMaker：用于自定义嵌入模型训练

RAG与Fine-tuning的比较：
- RAG适合需要频繁更新的知识和事实性查询
- Fine-tuning适合需要模型学习特定风格、格式或推理模式的场景
- 两者可以结合使用，先微调模型，再使用RAG提供最新信息
